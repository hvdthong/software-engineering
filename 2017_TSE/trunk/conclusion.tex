\section{Conclusion and Future Work}
\label{sec.conclusion}

In this paper, we put forward a novel multi-modal bug localization approach  named \underline{Net}work-clustered \underline{M}ulti-modal Bug \underline{L}ocalization (NetML). Deviating from the contemporary multi-modal localization approaches, NetML is able to achieve an effective bug localization through the interplay of two sets of latent parameters characterizing both bug reports and methods. It also features an adaptive learning procedure that stems from a strictly convex objective function formulation, thereby provides a sound theoretical guarantee on the uniqueness of the optimal solution.

We have extensively evaluated NetML on 157 real bugs from four different software projects (i.e., AspectJ, Ant, Lucene, and Rhino). Among the 157 bugs, NetML is able to successfully localize 46, 82, and 100 bugs when developers inspect the top 1, top 5, and top 10 methods, respectively. Compared to the best performing baseline (i.e., AML), NetML can successfully localize 48.39\%, 15.49\%, and 8.7\% more bugs when developers inspect the top 1, top 5, and top 10 methods, respectively. Furthermore, in terms of MAP, NetML outperforms the other baselines by 13.92\%. Based on the Wilcoxon signed-rank test using BH procedure, we show that the results of NetML are significantly better across the four projects, in terms of top 1, top 10, and MAP scores.

Although NetML offers a powerful bug localization approach, there remains room for improvement. For example, the current approach as well as the IR-based techniques capture both bug report and program element (method) using a simple bag-of-words (e.g., TF-IDF) representation, ignoring the inherent structure within the source codes of a program, such as function call and/or data dependencies. 
%only treated the methods as natural language by representing both the bug report and source code based on bag-of-words feature representations, and correlate the bug report and source code by measuring similarity in the same lexical feature space. 
In the future, we wish to consider a richer set of structural information within a program element, which carries additional semantics beyond the lexical terms. In particular, we would like to leverage both program structure information and lexical source code to localize potential bugs. We also plan to develop a more sophisticated technique, e.g., based on deep learning~\cite{Goodfellow2016}, to automatically learn the feature representation of bug reports and program elements.

%In this paper, we put forward a novel multi-modal bug localization approach named \underline{A}daptive \underline{M}ulti-modal bug \underline{L}ocalization (AML). Different from previous multi-modal approaches that are one-size-fits-all, our proposed approach can adapt itself to better localize each new bug report by tuning various weights learned from a set of training bug reports that are relevant to the new report. AML (in particular its \textit{AML}$^\textit{SuspWord}$ component) also leverages the concept of {\em suspicious words} (i.e., words that are associated to a bug) to better localize bugs. We have evaluated our proposed approach on 157 real bugs from 4 software systems. Our experiments highlight that, among the 157 bugs, AML can successfully localize 31, 71, and 92 bugs when developers inspect the top 1, top 5, and top 10 methods, respectively. Compared to the best performing baseline, AML can successfully localize 47.62\%, 31.48\%, and 27.78\% more bugs when developers inspect the top 1, top 5, and top 10 methods, respectively. Furthermore, in terms of MAP, AML outperforms the best baseline by 28.80\%.

%AML can successfully localize 31, 71, and 92 bugs when developers inspect the top 1, top 5, and top 10 methods, respectively. Compared to the best performing baseline (i.e., PROMESIR), AML can successfully localize 47.62\%, 31.48\%, and 27.78\% more bugs when developers inspect the top 1, top 5, and top 10 methods, respectively. Furthermore, in terms of MAP, AML outperforms the best baseline by 28.80\%. AML* is an extension framework of AML takes into account the similarity properties of bug reports and methods, and outperforms AML by 48.39\%, 15.49\%, and 8.7\% in the top 1, top 5, and top 10 methods. Moreover, AML* also outperforms AML by 13.9\% in terms of MAP scores.

%achieve an average MAP score of 0.237 and a success-count@1, success-count@5, and success-count@10 of 31, 72, and 92, respectively. These scores are better than the best performing baseline by 29.20\%, 47.62\%, 33.33\%, and 27.78\% for MAP, success-count@1, success-count@5, and success-rate@10, respectively.

%One of the components of AML (i.e., $AML^{SuspWord}$) leverages .  %Based on the observation of Parnin and Orso~\cite{ParninO11}, we also introduce and use the concept of word suspiciousness to identify buggy methods.
%To compute the suspiciousness of a method, our approach breaks the method into its constituent words, computes the suspiciousness scores of the constituent words, and combines these scores.
%Our proposed approach is thus analogous to a machine learning algorithm that breaks a data instance into its constituent features, computes the weights of these features, and combines the values of these features back to recommend a label for the data instance.

%In the future, we plan to improve the effectiveness of our proposed approach in terms of Top N and MAP scores. To reduce the threats to external validity, we also plan to investigate more bug reports from additional software systems. %which are implemented in different programming languages.

\vspace{0.2cm}\noindent{\bf Dataset and Codes.} Additional information on the 157 bugs used in the experiments can be found at \url{https://bitbucket.org/amlfse/amldata}. The codes of the NetML method can also be made available upon request.

%In the future, we plan to investigate more bug reports from additional software systems to increase generalizablity of our findings. {\color{red}We also plan to improve the effectiveness of our proposed approach further in order to achieve higher Top N score and Mean Average Precision. Information of the dataset is available at \url{google.com}.}
